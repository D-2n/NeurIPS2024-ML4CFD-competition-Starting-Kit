{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from lips import get_root_path\n",
    "# from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import AirfRANSDataSet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = 'Dataset'\n",
    "BENCHMARK_NAME = \"DEFAULT\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\"\n",
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\",\"benchmarks\",\"confAirfoil.ini\") \n",
    "\n",
    "directory_name='Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar model to the one in MLP_test_model.ipynb. However, here we have 128 nodes in hidden layer and using ReLU instead Tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train, test, epochs=50):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # model.eval()\n",
    "        # val_loss = 0.0\n",
    "        # with torch.no_grad():\n",
    "        #     for X_val, y_val in test:\n",
    "        #         X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        #         y_pred = model(X_val)\n",
    "        #         loss = criterion(y_pred, y_val)\n",
    "        #         val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss/len(train):.4f}\")\n",
    "        # print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(test):.4f} | Test Loss: {val_loss/len(test):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    # Deifinition of Input and Output attributes\n",
    "    X_attr = [\"x-position\", \"y-position\", \"x-inlet_velocity\", \"y-inlet_velocity\", \"x-normals\", \"y-normals\", \"distance_function\"]\n",
    "    y_attr = [\"x-velocity\", \"y-velocity\", \"pressure\", \"turbulent_viscosity\"]\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Looping through 10 items from the dataset to get the attributes\n",
    "    for i in range(1,35907642):\n",
    "        X.append(np.column_stack([dataset.data[attr][i] for attr in X_attr]))\n",
    "        y.append(np.column_stack([dataset.data[attr][i] for attr in y_attr]))\n",
    "\n",
    "    # Stacking the data to form a single matrix\n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = (\n",
    "        'x-position',\n",
    "        'y-position',\n",
    "        'x-inlet_velocity', \n",
    "        'y-inlet_velocity', \n",
    "        'distance_function', \n",
    "        'x-normals', \n",
    "        'y-normals', \n",
    "        'x-velocity', \n",
    "        'y-velocity', \n",
    "        'pressure', \n",
    "        'turbulent_viscosity',\n",
    "    )\n",
    "attr_x = attr_names[:7]\n",
    "attr_y = attr_names[7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from 2-Import_Airfoil_design_Dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_file = None #Convenient alternative but not required at this point\n",
    "dataset_name = \"my_dataset_test\"\n",
    "usecase_task = \"scarce\" #Four task are supported: 'full', 'scarce', 'reynolds', 'aoa'\n",
    "usecase_split = \"training\" #Describe which data subset within a task to be used, the other option is testing\n",
    "log_path = \"dataset_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = AirfRANSDataSet(config = configuration_file, \n",
    "                             name = dataset_name,\n",
    "                             task = usecase_task,\n",
    "                             split = usecase_split,\n",
    "                             attr_names = attr_names, \n",
    "                             attr_x = attr_x, \n",
    "                             attr_y = attr_y, \n",
    "                             log_path = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset (task: scarce, split: train): 100%|██████████| 200/200 [00:36<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "my_dataset.load(path=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35907642,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset.data['x-position'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data(my_dataset) # Took around 8 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 564570.6549\n",
      "Epoch 2/50 | Loss: 365984.4185\n",
      "Epoch 3/50 | Loss: 322295.1194\n",
      "Epoch 4/50 | Loss: 287690.9939\n",
      "Epoch 5/50 | Loss: 249313.2268\n",
      "Epoch 6/50 | Loss: 214248.8200\n",
      "Epoch 7/50 | Loss: 191472.1132\n",
      "Epoch 8/50 | Loss: 174811.0045\n",
      "Epoch 9/50 | Loss: 169234.3456\n",
      "Epoch 10/50 | Loss: 150391.4979\n",
      "Epoch 11/50 | Loss: 146287.5184\n",
      "Epoch 12/50 | Loss: 136238.8990\n",
      "Epoch 13/50 | Loss: 126794.0700\n",
      "Epoch 14/50 | Loss: 120513.0955\n",
      "Epoch 15/50 | Loss: 117777.5674\n",
      "Epoch 16/50 | Loss: 113314.9937\n",
      "Epoch 17/50 | Loss: 108756.0943\n",
      "Epoch 18/50 | Loss: 104055.8228\n",
      "Epoch 19/50 | Loss: 102635.6302\n",
      "Epoch 20/50 | Loss: 99981.7662\n",
      "Epoch 21/50 | Loss: 95522.7067\n",
      "Epoch 22/50 | Loss: 95588.2278\n",
      "Epoch 23/50 | Loss: 94317.6919\n",
      "Epoch 24/50 | Loss: 92798.3962\n",
      "Epoch 25/50 | Loss: 89484.8576\n",
      "Epoch 26/50 | Loss: 85487.8752\n",
      "Epoch 27/50 | Loss: 86601.1200\n",
      "Epoch 28/50 | Loss: 86663.4564\n",
      "Epoch 29/50 | Loss: 86594.6364\n",
      "Epoch 30/50 | Loss: 78555.1687\n",
      "Epoch 31/50 | Loss: 80814.3462\n",
      "Epoch 32/50 | Loss: 83696.9900\n",
      "Epoch 33/50 | Loss: 78755.3153\n",
      "Epoch 34/50 | Loss: 79900.2691\n",
      "Epoch 35/50 | Loss: 74848.6761\n",
      "Epoch 36/50 | Loss: 77979.5282\n",
      "Epoch 37/50 | Loss: 72851.6495\n",
      "Epoch 38/50 | Loss: 72720.2027\n",
      "Epoch 39/50 | Loss: 71820.8128\n",
      "Epoch 40/50 | Loss: 67597.4891\n",
      "Epoch 41/50 | Loss: 72689.0596\n",
      "Epoch 42/50 | Loss: 68165.4271\n",
      "Epoch 43/50 | Loss: 66628.7574\n",
      "Epoch 44/50 | Loss: 69768.3090\n",
      "Epoch 45/50 | Loss: 59490.5807\n",
      "Epoch 46/50 | Loss: 59664.1570\n",
      "Epoch 47/50 | Loss: 63024.7942\n",
      "Epoch 48/50 | Loss: 54170.3228\n",
      "Epoch 49/50 | Loss: 58606.3489\n",
      "Epoch 50/50 | Loss: 63589.9251\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train, test) # 220 min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
