{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from lips import get_root_path\n",
    "# from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import AirfRANSDataSet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = 'Dataset'\n",
    "BENCHMARK_NAME = \"DEFAULT\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\"\n",
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\",\"benchmarks\",\"confAirfoil.ini\") \n",
    "\n",
    "directory_name='Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=50, lr=1e-3):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                y_pred = model(X_val)\n",
    "                loss = criterion(y_pred, y_val)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(train_loader):.6f} | Val Loss: {val_loss/len(val_loader):.6f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    X_attr = [\"x-position\", \"y-position\", \"x-inlet_velocity\", \"y-inlet_velocity\", \"x-normals\", \"y-normals\", \"distance_function\"]\n",
    "    y_attr = [\"x-velocity\", \"y-velocity\", \"pressure\", \"turbulent_viscosity\"]\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(1,10):\n",
    "        X.append(np.column_stack([dataset.data[attr][i] for attr in X_attr]))\n",
    "        y.append(np.column_stack([dataset.data[attr][i] for attr in y_attr]))\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = (\n",
    "        'x-position',\n",
    "        'y-position',\n",
    "        'x-inlet_velocity', \n",
    "        'y-inlet_velocity', \n",
    "        'distance_function', \n",
    "        'x-normals', \n",
    "        'y-normals', \n",
    "        'x-velocity', \n",
    "        'y-velocity', \n",
    "        'pressure', \n",
    "        'turbulent_viscosity',\n",
    "    )\n",
    "attr_x = attr_names[:7]\n",
    "attr_y = attr_names[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_file = None #Convenient alternative but not required at this point\n",
    "dataset_name = \"my_dataset_test\"\n",
    "usecase_task = \"scarce\" #Four task are supported: 'full', 'scarce', 'reynolds', 'aoa'\n",
    "usecase_split = \"training\" #Describe which data subset within a task to be used, the other option is testing\n",
    "log_path = \"dataset_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = AirfRANSDataSet(config = configuration_file, \n",
    "                             name = dataset_name,\n",
    "                             task = usecase_task,\n",
    "                             split = usecase_split,\n",
    "                             attr_names = attr_names, \n",
    "                             attr_x = attr_x, \n",
    "                             attr_y = attr_y, \n",
    "                             log_path = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset (task: scarce, split: train): 100%|██████████| 200/200 [00:36<00:00,  5.54it/s]\n"
     ]
    }
   ],
   "source": [
    "my_dataset.load(path=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.21555805,  4.21557093,  3.99052548, ..., -2.13636851,\n",
       "       -2.14823389, -2.16010475])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset.data[\"x-position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 328.363312 | Val Loss: 315.561279\n",
      "Epoch 2/50 | Train Loss: 315.177704 | Val Loss: 302.677582\n",
      "Epoch 3/50 | Train Loss: 302.291046 | Val Loss: 289.427948\n",
      "Epoch 4/50 | Train Loss: 289.031219 | Val Loss: 276.001648\n",
      "Epoch 5/50 | Train Loss: 275.601746 | Val Loss: 261.712982\n",
      "Epoch 6/50 | Train Loss: 261.306366 | Val Loss: 246.266724\n",
      "Epoch 7/50 | Train Loss: 245.864716 | Val Loss: 229.595932\n",
      "Epoch 8/50 | Train Loss: 229.194305 | Val Loss: 211.650818\n",
      "Epoch 9/50 | Train Loss: 211.252518 | Val Loss: 192.639771\n",
      "Epoch 10/50 | Train Loss: 192.248947 | Val Loss: 172.640686\n",
      "Epoch 11/50 | Train Loss: 172.266815 | Val Loss: 151.939362\n",
      "Epoch 12/50 | Train Loss: 151.590973 | Val Loss: 130.824799\n",
      "Epoch 13/50 | Train Loss: 130.500977 | Val Loss: 109.563049\n",
      "Epoch 14/50 | Train Loss: 109.273392 | Val Loss: 88.723915\n",
      "Epoch 15/50 | Train Loss: 88.482239 | Val Loss: 69.020523\n",
      "Epoch 16/50 | Train Loss: 68.836746 | Val Loss: 51.185768\n",
      "Epoch 17/50 | Train Loss: 51.071552 | Val Loss: 35.881989\n",
      "Epoch 18/50 | Train Loss: 35.845913 | Val Loss: 23.698084\n",
      "Epoch 19/50 | Train Loss: 23.741552 | Val Loss: 15.107885\n",
      "Epoch 20/50 | Train Loss: 15.225059 | Val Loss: 10.360441\n",
      "Epoch 21/50 | Train Loss: 10.541288 | Val Loss: 9.296919\n",
      "Epoch 22/50 | Train Loss: 9.529114 | Val Loss: 11.155939\n",
      "Epoch 23/50 | Train Loss: 11.421961 | Val Loss: 14.653420\n",
      "Epoch 24/50 | Train Loss: 14.933493 | Val Loss: 18.155476\n",
      "Epoch 25/50 | Train Loss: 18.426924 | Val Loss: 20.267883\n",
      "Epoch 26/50 | Train Loss: 20.506821 | Val Loss: 20.390450\n",
      "Epoch 27/50 | Train Loss: 20.575693 | Val Loss: 18.708122\n",
      "Epoch 28/50 | Train Loss: 18.825924 | Val Loss: 15.841892\n",
      "Epoch 29/50 | Train Loss: 15.885497 | Val Loss: 12.572598\n",
      "Epoch 30/50 | Train Loss: 12.542802 | Val Loss: 9.544368\n",
      "Epoch 31/50 | Train Loss: 9.448255 | Val Loss: 7.139190\n",
      "Epoch 32/50 | Train Loss: 6.987958 | Val Loss: 5.508724\n",
      "Epoch 33/50 | Train Loss: 5.315388 | Val Loss: 4.639717\n",
      "Epoch 34/50 | Train Loss: 4.417280 | Val Loss: 4.400615\n",
      "Epoch 35/50 | Train Loss: 4.160900 | Val Loss: 4.590545\n",
      "Epoch 36/50 | Train Loss: 4.343874 | Val Loss: 4.992467\n",
      "Epoch 37/50 | Train Loss: 4.747360 | Val Loss: 5.410742\n",
      "Epoch 38/50 | Train Loss: 5.173886 | Val Loss: 5.691103\n",
      "Epoch 39/50 | Train Loss: 5.468015 | Val Loss: 5.734918\n",
      "Epoch 40/50 | Train Loss: 5.529820 | Val Loss: 5.499185\n",
      "Epoch 41/50 | Train Loss: 5.315392 | Val Loss: 4.995908\n",
      "Epoch 42/50 | Train Loss: 4.836035 | Val Loss: 4.282891\n",
      "Epoch 43/50 | Train Loss: 4.148722 | Val Loss: 3.447977\n",
      "Epoch 44/50 | Train Loss: 3.340561 | Val Loss: 2.592776\n",
      "Epoch 45/50 | Train Loss: 2.512397 | Val Loss: 1.817771\n",
      "Epoch 46/50 | Train Loss: 1.763861 | Val Loss: 1.208040\n",
      "Epoch 47/50 | Train Loss: 1.179100 | Val Loss: 0.818686\n",
      "Epoch 48/50 | Train Loss: 0.812259 | Val Loss: 0.663785\n",
      "Epoch 49/50 | Train Loss: 0.676434 | Val Loss: 0.714601\n",
      "Epoch 50/50 | Train Loss: 0.741957 | Val Loss: 0.908180\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
