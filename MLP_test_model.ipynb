{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from lips import get_root_path\n",
    "# from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import AirfRANSDataSet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = 'Dataset'\n",
    "BENCHMARK_NAME = \"DEFAULT\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\"\n",
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\",\"benchmarks\",\"confAirfoil.ini\") \n",
    "\n",
    "directory_name='Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar model to the one in MLP_test_model.ipynb. However, here we have 128 nodes in hidden layer and using ReLU instead Tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train, test, epochs=50):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # model.eval()\n",
    "        # val_loss = 0.0\n",
    "        # with torch.no_grad():\n",
    "        #     for X_val, y_val in test:\n",
    "        #         X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        #         y_pred = model(X_val)\n",
    "        #         loss = criterion(y_pred, y_val)\n",
    "        #         val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss/len(train):.4f}\")\n",
    "        # print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(test):.4f} | Test Loss: {val_loss/len(test):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    # Deifinition of Input and Output attributes\n",
    "    X_attr = [\"x-position\", \"y-position\", \"x-inlet_velocity\", \"y-inlet_velocity\", \"x-normals\", \"y-normals\", \"distance_function\"]\n",
    "    y_attr = [\"x-velocity\", \"y-velocity\", \"pressure\", \"turbulent_viscosity\"]\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Looping through 10 items from the dataset to get the attributes\n",
    "    for i in range(1,10):\n",
    "        X.append(np.column_stack([dataset.data[attr][i] for attr in X_attr]))\n",
    "        y.append(np.column_stack([dataset.data[attr][i] for attr in y_attr]))\n",
    "\n",
    "    # Stacking the data to form a single matrix\n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = (\n",
    "        'x-position',\n",
    "        'y-position',\n",
    "        'x-inlet_velocity', \n",
    "        'y-inlet_velocity', \n",
    "        'distance_function', \n",
    "        'x-normals', \n",
    "        'y-normals', \n",
    "        'x-velocity', \n",
    "        'y-velocity', \n",
    "        'pressure', \n",
    "        'turbulent_viscosity',\n",
    "    )\n",
    "attr_x = attr_names[:7]\n",
    "attr_y = attr_names[7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from 2-Import_Airfoil_design_Dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_file = None #Convenient alternative but not required at this point\n",
    "dataset_name = \"my_dataset_test\"\n",
    "usecase_task = \"scarce\" #Four task are supported: 'full', 'scarce', 'reynolds', 'aoa'\n",
    "usecase_split = \"training\" #Describe which data subset within a task to be used, the other option is testing\n",
    "log_path = \"dataset_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = AirfRANSDataSet(config = configuration_file, \n",
    "                             name = dataset_name,\n",
    "                             task = usecase_task,\n",
    "                             split = usecase_split,\n",
    "                             attr_names = attr_names, \n",
    "                             attr_x = attr_x, \n",
    "                             attr_y = attr_y, \n",
    "                             log_path = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset (task: scarce, split: train): 100%|██████████| 200/200 [00:43<00:00,  4.59it/s]\n"
     ]
    }
   ],
   "source": [
    "my_dataset.load(path=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 344.7481\n",
      "Epoch 2/50 | Loss: 324.4673\n",
      "Epoch 3/50 | Loss: 306.4139\n",
      "Epoch 4/50 | Loss: 290.2108\n",
      "Epoch 5/50 | Loss: 274.3653\n",
      "Epoch 6/50 | Loss: 257.5312\n",
      "Epoch 7/50 | Loss: 240.0352\n",
      "Epoch 8/50 | Loss: 221.7873\n",
      "Epoch 9/50 | Loss: 202.7572\n",
      "Epoch 10/50 | Loss: 183.2710\n",
      "Epoch 11/50 | Loss: 163.7333\n",
      "Epoch 12/50 | Loss: 143.9605\n",
      "Epoch 13/50 | Loss: 124.3269\n",
      "Epoch 14/50 | Loss: 104.9135\n",
      "Epoch 15/50 | Loss: 86.1161\n",
      "Epoch 16/50 | Loss: 68.4313\n",
      "Epoch 17/50 | Loss: 52.3972\n",
      "Epoch 18/50 | Loss: 38.3190\n",
      "Epoch 19/50 | Loss: 26.7806\n",
      "Epoch 20/50 | Loss: 17.9932\n",
      "Epoch 21/50 | Loss: 12.1607\n",
      "Epoch 22/50 | Loss: 9.2318\n",
      "Epoch 23/50 | Loss: 8.8208\n",
      "Epoch 24/50 | Loss: 10.1630\n",
      "Epoch 25/50 | Loss: 12.2527\n",
      "Epoch 26/50 | Loss: 14.1637\n",
      "Epoch 27/50 | Loss: 15.3028\n",
      "Epoch 28/50 | Loss: 15.4447\n",
      "Epoch 29/50 | Loss: 14.7049\n",
      "Epoch 30/50 | Loss: 13.3403\n",
      "Epoch 31/50 | Loss: 11.6280\n",
      "Epoch 32/50 | Loss: 9.8306\n",
      "Epoch 33/50 | Loss: 8.1594\n",
      "Epoch 34/50 | Loss: 6.7667\n",
      "Epoch 35/50 | Loss: 5.6706\n",
      "Epoch 36/50 | Loss: 4.8613\n",
      "Epoch 37/50 | Loss: 4.2867\n",
      "Epoch 38/50 | Loss: 3.8603\n",
      "Epoch 39/50 | Loss: 3.5191\n",
      "Epoch 40/50 | Loss: 3.2076\n",
      "Epoch 41/50 | Loss: 2.8916\n",
      "Epoch 42/50 | Loss: 2.5652\n",
      "Epoch 43/50 | Loss: 2.2427\n",
      "Epoch 44/50 | Loss: 1.9503\n",
      "Epoch 45/50 | Loss: 1.7169\n",
      "Epoch 46/50 | Loss: 1.5651\n",
      "Epoch 47/50 | Loss: 1.5004\n",
      "Epoch 48/50 | Loss: 1.5093\n",
      "Epoch 49/50 | Loss: 1.5649\n",
      "Epoch 50/50 | Loss: 1.6298\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
